# -*- coding: utf-8 -*-
"""Pytorch-RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/108X-lihQTdx5uW2oRwvCFlevmfFqrUln

# Data preprocessing

## Load data
"""

academy_titles = []
job_titles = []

with open("academy_titles.txt", "r", encoding= "utf-8") as f:
    for l in f:
      academy_titles.append(l.strip()) # remove spaces of head and tail

with open("job_titles.txt", "r", encoding= "utf-8") as f:
    for l in f:
      job_titles.append(l.strip()) # remove spaces of head and tail

print(academy_titles[:5])
print(job_titles[:5])

"""## Word tokenizing"""

char_set = set()

for title in academy_titles:
    for char in title:
        char_set.add(char)

for title in job_titles:
    for char in title:
        char_set.add(char)

print(char_set)
print(len(char_set))

char_list = list(char_set) # typecasting: set to list
n_chars = len(char_list) + 1 # +1 for non-existent characters(<unk>)

"""## Convert title string to a tensor of character indices"""

import torch

def title_to_tensor(title):
    tensor = torch.zeros(len(title), dtype=torch.long) # initialize a tensor of zeros with the length of the title

    for li, char in enumerate(title):
      try:
        ind = char_list.index(char) + 1 # +1 to differentiate "0" in index and "0" in zero tensor
      except ValueError:
        ind = n_chars - 1 # -1 to ensure the highest index for unknown characters are reserved
      tensor[li] = ind

    return tensor

"""## Build dataset & Split independent variables and dependent variable"""

all_data = []

for l in academy_titles:
    all_data.append((title_to_tensor(l), torch.tensor([0], dtype=torch.long)))

for l in job_titles:
    all_data.append((title_to_tensor(l), torch.tensor([1], dtype=torch.long)))

print(all_data[:5])

"""## Split training set and testing set"""

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)
print(len(train_data))
print(len(test_data))

"""## Define collate function to handle variable-length sequences"""

from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    titles, labels = zip(*batch)
    titles_padded = pad_sequence(titles, batch_first=True, padding_value=0) # make all sequences the same length
    labels = torch.cat(labels) # concatenate labels
    return titles_padded, labels

"""# Neural network processing

## Check if there is GPU to use
"""

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("There are %d GPU(s) available." % torch.cuda.device_count())
else:
    device = torch.device("cpu")
    print("No GPU available, using CPU instead.")

"""## Define model"""

import torch.nn as nn
import torch.nn.init as init
import torch.optim as optim

class RnnModel(nn.Module):

    # define frameworks of each neural layer
    def __init__(self, word_count, embedding_size, hidden_size, output_size):
        super(RnnModel, self).__init__()

        # define neural layers
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(word_count, embedding_size) # embedding layer
        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True) # RNN layer
        self.fc = nn.Linear(hidden_size, output_size) # full connected layer

        # define weight initializers of each layer(default)
        init.xavier_normal_(self.fc.weight)

    # define forward propagation function to connect layers(including activation function)
    def forward(self, input_tensor):
      word_vector = self.embedding(input_tensor)
      output, (hidden, _) = self.rnn(word_vector) # LSTM returns output: (hidden, cell), we need hidden state of the last time step.
      output = self.fc(hidden[-1]) # extract the hidden state of the last layer
      return output

# set hyperparameter values
word_count = n_chars
embedding_size = 200
hidden_size = 10
output_size = 2

# initialize model
model = RnnModel(word_count, embedding_size, hidden_size, output_size).to(device)

# save model for restful api
torch.save(model.state_dict(), "model.pt")

# define loss function
criterion = nn.CrossEntropyLoss()

# define optimizer
optimizer = optim.Adam(model.parameters(), lr=0.005)

"""## Model training & evaluation"""

from torch.utils.data import DataLoader

batch_size = 10

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

epochs = 5
for epoch in range(epochs):
  model.train() # weights can be modified

  correct = 0
  total = 0

  for X_batch, Y_batch in train_loader:
    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
    optimizer.zero_grad() # return zero of every previous batch

    Y_pred = model(X_batch)
    loss = criterion(Y_pred, Y_batch.squeeze())
    loss.backward() # calculate gradient(min loss weights)
    optimizer.step() # update weights

    _, predicted = torch.max(Y_pred.data, 1) # get the index of max value in each row of axis 1
    total += Y_batch.size(0) # get the number of samples
    correct += (predicted == Y_batch.squeeze()).sum().item()

  accuracy = correct / total
  print(f"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}")

"""## Test evaluation"""

model.eval() # weights cannot be modified(frozen)

y_true = []
y_pred = []

with torch.no_grad(): # close the gradient calculation mechanism
  correct = 0
  total = 0

  for X_batch, Y_batch in test_loader:
    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)

    Y_pred = model(X_batch)
    _, predicted = torch.max(Y_pred.data, 1)
    total += Y_batch.size(0)
    correct += (predicted == Y_batch.squeeze()).sum().item()
    y_true.extend(Y_batch.squeeze().cpu().numpy())
    y_pred.extend(predicted.cpu().numpy())
  print(f"Test Acc: {correct / total:.4f}")

"""## Answer prediction"""

import pandas as pd

results_df = pd.DataFrame({
    "Y_true": y_true,
    "Y_pred": y_pred
})
print(results_df)