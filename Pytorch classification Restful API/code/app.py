# -*- coding: utf-8 -*-
"""Pytorch-RNN(Restful API).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ml3f1UXaPnO--3KxaIPcZV6uhus6_6eg

# Data preprocessing

## Load data
"""

academy_titles = []
job_titles = []

with open("academy_titles.txt", "r", encoding= "utf-8") as f:
    for l in f:
      academy_titles.append(l.strip()) # remove spaces of head and tail

with open("job_titles.txt", "r", encoding= "utf-8") as f:
    for l in f:
      job_titles.append(l.strip()) # remove spaces of head and tail

print(academy_titles[:5])
print(job_titles[:5])

"""## Word tokenizing"""

char_set = set()

for title in academy_titles:
    for char in title:
        char_set.add(char)

for title in job_titles:
    for char in title:
        char_set.add(char)

print(char_set)
print(len(char_set))

char_list = list(char_set) # typecasting: set to list
n_chars = len(char_list) + 1 # +1 for non-existent characters(<unk>)

"""# API setting

## Import library
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import torch.nn as nn
import torch.nn.init as init

"""## Load model"""

app = FastAPI()

# Model's framework(written first due to the importance)
class RnnModel(nn.Module):

    # define frameworks of each neural layer
    def __init__(self, word_count, embedding_size, hidden_size, output_size):
        super(RnnModel, self).__init__()

        # define neural layers
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(word_count, embedding_size) # embedding layer
        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True) # RNN layer
        self.fc = nn.Linear(hidden_size, output_size) # full connected layer

        # define weight initializers of each layer(default)
        init.xavier_normal_(self.fc.weight)

    # define forward propagation function to connect layers(including activation function)
    def forward(self, input_tensor):
      word_vector = self.embedding(input_tensor)
      output, (hidden, _) = self.rnn(word_vector) # LSTM returns output: (hidden, cell), we need hidden state of the last time step.
      output = self.fc(hidden[-1]) # extract the hidden state of the last layer
      return output

word_count = n_chars
embedding_size = 200
hidden_size = 10
output_size = 2

model = RnnModel(word_count, embedding_size, hidden_size, output_size)
model.load_state_dict(torch.load("model.pt", map_location=torch.device("cpu")))
model.eval() # evaluation mode

# Convert title string to a tensor of character indices
def title_to_tensor(title):
    tensor = torch.zeros(len(title), dtype=torch.long) # initialize a tensor of zeros with the length of the title

    for li, char in enumerate(title):
      try:
        ind = char_list.index(char) + 1 # +1 to differentiate "0" in index and "0" in zero tensor
      except ValueError:
        ind = n_chars - 1 # -1 to ensure the highest index for unknown characters are reserved
      tensor[li] = ind

    return tensor.unsqueeze(0) # add a batch dimension

"""## Define API structure"""

# for asking data verification
class TitleRequest(BaseModel):
    title: str

# prediction endpoint
@app.post("/predict/title")
async def predict_title(data: TitleRequest):
    title = data.title
    if not title:
        raise HTTPException(status_code=400, detail="Title must not be empty")

    tensor = title_to_tensor(title)

    with torch.no_grad():
        output = model(tensor)
        probabilities = torch.softmax(output, dim=1).cpu().numpy() # get predict probabilities of the categories 
        _, predicted = torch.max(output, 1) 
        label = "academy title" if predicted.item() == 0 else "job title"

    return {
        "title": title,
        "label": label,
        "probabilities": probabilities.tolist()
    }

# health checker
@app.get("/health")
async def health_check():
    return {"status": "API is working"}